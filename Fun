def sf_validate_pre_checks(kwargs): """ Performs pre-validation checks: - Table availability in the outbound dataset - Table emptiness - Logging of control outcomes Supports both GCS-to-BQ and BQ-to-BQ pipelines. """ from datetime import datetime as dt2, timezone import sys import logging

ti = kwargs['task_instance']
dimc_pre_check_response = {
    "DQ_TABLE_AVAILABILITY_CHECK": {"Execution": "not-performed", "Result": "not-applicable"},
    "DQ_TABLE_EMPTY_CHECK": {"Execution": "not-performed", "Result": "not-applicable"}
}
pre_check_response = {
    "uuid": "",
    "trx_id": "",
    "project_id": "",
    "app_id": "",
    "created_by": "",
    "control_type": "pre-controls",
    "source_type": "BigQuery",
    "source_name": "",
    "source_object": "",
    "target_type": "BigQuery",
    "target_name": "",
    "target_object": "",
    "library_version": 1.0,
    "control_id": "",
    "control_name": "",
    "control_status": "",
    "log_info": ""
}
try:
    start_time = int(dt2.now(timezone.utc).timestamp() * 1000)

    # Fetch common variables
    bucket = ti.xcom_pull(task_ids='run_time_variables', key='source_bucket')
    lane = ti.xcom_pull(task_ids='run_time_variables', key='lane')
    sfr_project_id = ti.xcom_pull(task_ids='run_time_variables', key='sfr_project_id')
    config_data = ti.xcom_pull(task_ids='run_time_variables', key='config_data')
    staging_dataset = ti.xcom_pull(task_ids='run_time_variables', key='sfr_outbound_dataset').split('.')[1]
    sfr_dlp_bq = ti.xcom_pull(task_ids='run_time_variables', key='sfr_dlp_bq')

    # Set pre-check metadata
    pre_check_response["uuid"] = ti.xcom_pull(task_ids='run_time_variables', key='uuid')
    pre_check_response["trx_id"] = config_data['trx_id']
    pre_check_response["project_id"] = config_data['project_id']
    pre_check_response["app_id"] = config_data['app_id']
    pre_check_response["created_by"] = config_data['app_id'] + "_composer_job"
    pre_check_response["source_name"] = config_data['source_name']
    pre_check_response["source_object"] = config_data['sfr_outbound_dataset']
    pre_check_response["target_name"] = config_data['target_name']
    pre_check_response["target_object"] = config_data['target_object']
    pre_check_response["start_time"] = start_time

    avro_files = []

    if sfr_dlp_bq == 'enable':
        toc_file = ti.xcom_pull(task_ids='run_time_variables', key='toc_file')
        files_to_archive = toc_file.replace(".toc", "")
        avro_files_dict = read_gcs_file(bucket, toc_file)["dataTransfered"]["dataFile"]
        avro_files = [item["name"] for item in avro_files_dict]
    else:
        # In BQ to BQ case, assume tables are passed directly from previous step
        avro_files = ti.xcom_pull(task_ids='run_time_variables', key='staging_tables')

    part_tables = [file.replace('.avro', '') for file in avro_files] if sfr_dlp_bq == 'enable' else avro_files

    staging_tables = []
    empty_tables = []
    missing_staging_tables = []

    for part_table in part_tables:
        is_table_available = sf_table_availability_check(staging_dataset, part_table)
        if is_table_available:
            is_table_not_empty = sf_table_emptiness_check(staging_dataset, part_table)
            if is_table_not_empty:
                staging_tables.append(part_table)
            else:
                empty_tables.append(part_table)
        else:
            missing_staging_tables.append(part_table)

    if missing_staging_tables:
        pre_check_response["control_id"] = "DQ_TAVC"
        pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
        pre_check_response["control_status"] = "failed"
        pre_check_response["log_info"] = f"tables not available {missing_staging_tables}"
        pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
        kwargs['ti'].xcom_push(key='missing_staging_tables', value=missing_staging_tables)
        Logging.info(f"tables not available {missing_staging_tables}")
        sf_publish_logs(sfr_project_id, lane, pre_check_response)
        dimc_pre_check_response["DQ_TABLE_AVAILABILITY_CHECK"]["Execution"] = 'performed'
        dimc_pre_check_response["DQ_TABLE_AVAILABILITY_CHECK"]["Result"] = pre_check_response["log_info"]
        kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)
        sys.exit(1)
    else:
        pre_check_response["control_id"] = "DQ_TAVC"
        pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
        pre_check_response["control_status"] = "success"
        pre_check_response["log_info"] = "all the staging tables available in the sf outbound dataset"
        pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
        sf_publish_logs(sfr_project_id, lane, pre_check_response)
        dimc_pre_check_response["DQ_TABLE_AVAILABILITY_CHECK"]["Execution"] = 'performed'
        dimc_pre_check_response["DQ_TABLE_AVAILABILITY_CHECK"]["Result"] = "success"

    if empty_tables:
        pre_check_response["control_id"] = "DQ_TEMC"
        pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
        pre_check_response["control_status"] = "failed"
        pre_check_response["log_info"] = f"tables empty {empty_tables}"
        pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
        kwargs['ti'].xcom_push(key='empty_tables', value=empty_tables)
        Logging.info(f"tables empty {empty_tables}")
        sf_publish_logs(sfr_project_id, lane, pre_check_response)
        dimc_pre_check_response["DQ_TABLE_EMPTY_CHECK"]["Execution"] = 'performed'
        dimc_pre_check_response["DQ_TABLE_EMPTY_CHECK"]["Result"] = pre_check_response["log_info"]
        kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)
        sys.exit(1)
    else:
        pre_check_response["control_id"] = "DQ_TEMC"
        pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
        pre_check_response["control_status"] = "success"
        pre_check_response["log_info"] = "all the staging tables having data in sf outbound dataset"
        pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
        sf_publish_logs(sfr_project_id, lane, pre_check_response)
        dimc_pre_check_response["DQ_TABLE_EMPTY_CHECK"]["Execution"] = 'performed'
        dimc_pre_check_response["DQ_TABLE_EMPTY_CHECK"]["Result"] = "success"
        kwargs['ti'].xcom_push(key='staging_tables', value=staging_tables)

    kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)
except Exception as e:
    Logging.info("Exception occurred while trying to execute sf_validate_pre_checks with exception " + str(e))
    kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)
    sys.exit(1)

return dimc_pre_check_response


def dimc_pre_check(**kwargs):
    """
    Skip all validations and return dummy success result
    to allow BQ-to-BQ load to run for testing.
    """
    try:
        ti = kwargs['task_instance']

        # Directly fetch passed staging tables from run_time_variables
        staging_tables = ti.xcom_pull(task_ids='run_time_variables', key='staging_tables')

        logging.info(f"Skipping validations. Staging tables passed: {staging_tables}")

        # Prepare dummy pre-check response
        pre_check_response = {
            "DQ_TABLE_AVAILABILITY_CHECK": {"Execution": "skipped", "Result": "not-applicable"},
            "DQ_TABLE_EMPTY_CHECK": {"Execution": "skipped", "Result": "not-applicable"},
            "DQ_DATA_RECONCILIATION_CHECK": {"Execution": "skipped", "Result": "not-applicable"}
        }

        # Push dummy responses for downstream tasks
        ti.xcom_push(key='staging_tables', value=staging_tables)
        ti.xcom_push(key='dimc_pre_check_response', value=pre_check_response)

        return pre_check_response

    except Exception as e:
        logging.info(f"Error while skipping pre-checks: {e}")
        sys.exit(1)
...
.




def data_load_task(**kwargs):
    """
    Unified function to load data either from GCS to BQ or BQ to BQ based on `source_type`.
    """
    try:
        ti = kwargs['task_instance']
        source_type = ti.xcom_pull(task_ids='run_time_variables', key='source_type')

        if source_type and source_type.lower() == 'bigquery':
            logging.info("Detected source_type as BigQuery, calling data_load_bq_bq...")
            return data_load_bq_bq(**kwargs)

        # Else proceed with GCS-to-BQ logic
        logging.info("Proceeding with GCS to BQ load.")
        
        data_files = ti.xcom_pull(task_ids="list_data_files", key='part_files')
        source_bucket = ti.xcom_pull(task_ids='run_time_variables', key='source_bucket')
        target_dataset = ti.xcom_pull(task_ids='run_time_variables', key='target_dataset')
        target_name = ti.xcom_pull(task_ids='run_time_variables', key='target_name')
        write_mode = ti.xcom_pull(task_ids='run_time_variables', key='write_mode')
        data_load_pre_check = ti.xcom_pull(task_ids='run_time_variables', key='data_load_pre_check')
        pre_check_response = ti.xcom_pull(task_ids='dimc_pre_check')

        controls_failed = [key for key, value in pre_check_response.items() if value['Result'] not in ['success', 'not-applicable']]
        if controls_failed and data_load_pre_check == 'ABORT':
            logging.info(f"dimc pre-checks {controls_failed} have failed. Aborting the subsequent loads")
            sys.exit(1)

        data_etl_load_task = GCSToBigQueryOperator(
            task_id='data_etl_load_task',
            bucket=source_bucket,
            source_objects=data_files,
            destination_project_dataset_table=f"{target_dataset}.{target_name}",
            schema_fields=get_schema_fields(target_dataset, target_name),
            source_format='AVRO',
            autodetect=False,
            write_disposition=load_type(target_dataset, target_name, write_mode),
            create_disposition='CREATE_NEVER',
            src_fmt_configs={"useAvroLogicalTypes": True}
        )
        data_etl_load_task.execute(context=kwargs)

    except Exception as e:
        logging.info(f"An error occurred in the data_load_task: {e}")


        sys.exit(1)


........

def sf_validate_pre_checks(**kwargs):
    from datetime import datetime as dt2
    import logging
    import sys

    pre_check_response = {}
    staging_tables = []
    part_tables = []

    try:
        ti = kwargs['ti']
        config_data = ti.xcom_pull(task_ids='run_time_variables', key='config_data')
        sfr_dlp_bq = config_data.get("sfr_dlp_bq", "disable")
        sfr_outbound_dataset = config_data.get("sfr_outbound_dataset", "")
        target_dataset = config_data["target_dataset"]
        target_name = config_data["target_name"]

        start_time = dt2.now().timestamp()

        pre_check_response["project_id"] = config_data["project_id"]
        pre_check_response["app_id"] = config_data["app_id"]
        pre_check_response["created_by"] = config_data["app_id"] + "_composer_job"
        pre_check_response["source_name"] = config_data["source_name"]
        pre_check_response["source_object"] = config_data["sfr_outbound_dataset"]
        pre_check_response["target_name"] = config_data["target_name"]
        pre_check_response["target_object"] = config_data["target_object"]
        pre_check_response["start_time"] = start_time

        avro_files = []
        if sfr_dlp_bq == "disable":
            toc_file = ti.xcom_pull(task_ids='run_time_variables', key='toc_file')
            source_bucket = ti.xcom_pull(task_ids='run_time_variables', key='source_bucket')
            avro_files_dict = read_gcs_file(source_bucket, toc_file)["dataTransfered"]["dataFile"]
            avro_files = [item["name"] for item in avro_files_dict]
            part_tables = [file.replace(".avro", "") for file in avro_files]
            files_to_archive = toc_file.replace(".toc", "")
        else:
            # BQ-to-BQ case
            part_tables = ti.xcom_pull(task_ids='run_time_variables', key='staging_tables') or []
            staging_tables = part_tables
            logging.info(f"Available part tables to load: {staging_tables}")

        logging.info(f"List of data files: {avro_files}")
        logging.info(f"Config data: {config_data}")

        empty_tables = []
        missing_staging_tables = []

        # ✅ Safely skip validation if part_tables is empty (BQ-to-BQ)
        if part_tables:
            for part_table in part_tables:
                table_id = f"{sfr_outbound_dataset}.{part_table}"
                is_table_available = sf_table_availability_check(sfr_outbound_dataset, part_table)

                if is_table_available:
                    is_table_not_empty = sf_table_emptiness_check(sfr_outbound_dataset, part_table)
                    if is_table_not_empty:
                        staging_tables.append(part_table)
                    else:
                        empty_tables.append(part_table)
                else:
                    missing_staging_tables.append(part_table)

            if missing_staging_tables:
                pre_check_response["control_id"] = "DQ_TAVC"
                pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
                pre_check_response["control_status"] = "failed"
                pre_check_response["log_info"] = f"Tables not available: {missing_staging_tables}"
                pre_check_response["end_time"] = int(dt2.now().timestamp() * 1000)

                kwargs['ti'].xcom_push(key='missing_staging_tables', value=missing_staging_tables)
                logging.info(f"Tables not available: {missing_staging_tables}")
                sf_publish_logs(config_data["project_id"], config_data["lane"], pre_check_response)

                dimc_pre_check_response = {
                    "DQ_TABLE_AVAILABILITY_CHECK": {
                        "Execution": "performed",
                        "Result": "failed"
                    }
                }
                kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)
                sys.exit(1)
            else:
                pre_check_response["control_id"] = "DQ_IEVC"
                pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
                pre_check_response["control_status"] = "success"
                pre_check_response["log_info"] = "All the staging tables having data in sf_outbound_dataset"
                pre_check_response["end_time"] = int(dt2.now().timestamp() * 1000)

                sf_publish_logs(config_data["project_id"], config_data["lane"], pre_check_response)
                dimc_pre_check_response = {
                    "DQ_TABLE_EMPTY_CHECK": {
                        "Execution": "performed",
                        "Result": "success"
                    }
                }
                ti.xcom_push(key='staging_tables', value=staging_tables)
                logging.info(f"Available part tables to load: {staging_tables}")
                kwargs['ti'].xcom_push(key='dimc_pre_check_response', value=dimc_pre_check_response)

        else:
            logging.info("No part tables to validate — assuming BQ-to-BQ load or empty input.")
            kwargs['ti'].xcom_push(key='staging_tables', value=staging_tables)
            kwargs['ti'].xcom_push(key='dimc_pre_check_response', value={})

        return pre_check_response

    except Exception as e:
        logging.info(f"Exception occurred while trying to execute sf_validate_pre_checks with exception {str(e)}")
        kwargs['ti'].xcom_push(key='dimc_pre_check_response', value={})
        sys.exit(1)
