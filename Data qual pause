from __future__ import annotations

import json
import time
from datetime import datetime, timedelta

import requests
from airflow import DAG
from airflow.operators.python import PythonOperator
from google.auth import default
from google.auth.transport.requests import Request

# -------------------------------------------------------------------
# CONFIG
# -------------------------------------------------------------------
PROJECT_ID = "sandbox-corp-odin-dev1-f930"
REGION = "us-central1"

DATASET = "ref_data_src"
TABLE = "t_risk_driver"

# Dataplex DataScan ID (must be lowercase + dashes/underscores)
SCAN_ID = "dq-risk-driver-airflow"

# -------------------------------------------------------------------
# AUTH HELPERS
# -------------------------------------------------------------------
def get_auth_headers():
    credentials, _ = default(scopes=["https://www.googleapis.com/auth/cloud-platform"])
    credentials.refresh(Request())
    return {
        "Authorization": f"Bearer {credentials.token}",
        "Content-Type": "application/json",
    }

# -------------------------------------------------------------------
# DQ RULES (HARDCODED FOR NOW)
# -------------------------------------------------------------------
def build_dq_scan_body():
    rules = [
        {
            "column": "ODIN_RISK_DRIVER_ID",
            "nonNullExpectation": {},
            "dimension": "COMPLETENESS",
            "threshold": 1.0,
        },
        {
            "column": "ODIN_RISK_DRIVER_ID",
            "uniquenessExpectation": {},
            "dimension": "UNIQUENESS",
            "threshold": 1.0,
        },
        {
            "column": "RISK_DRIVER_LEVEL_2_RISK_DRIVER_TAXONOMY_ID",
            "rangeExpectation": {"minValue": "0"},
            "dimension": "VALIDITY",
        },
        {
            "column": "RISK_DRIVER_LEVEL_3_SEQUENCE",
            "rangeExpectation": {"minValue": "0"},
            "dimension": "VALIDITY",
        },
        {
            "column": "LAST_LOADED_DT",
            "nonNullExpectation": {},
            "dimension": "COMPLETENESS",
            "threshold": 1.0,
        },
    ]

    body = {
        "data": {
            "resource": (
                f"//bigquery.googleapis.com/projects/{PROJECT_ID}"
                f"/datasets/{DATASET}/tables/{TABLE}"
            )
        },
        "dataQualitySpec": {"rules": rules},
    }
    return body

# -------------------------------------------------------------------
# PYTHON CALLABLES
# -------------------------------------------------------------------
def create_or_update_scan(**context):
    """
    Idempotent:
    - If DataScan exists -> do nothing
    - Else create it and wait for the long-running operation to finish
    """
    headers = get_auth_headers()

    # 1) Check if DataScan already exists
    get_url = (
        f"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}"
        f"/locations/{REGION}/dataScans/{SCAN_ID}"
    )
    resp_get = requests.get(get_url, headers=headers)
    if resp_get.status_code == 200:
        print("DataScan already exists, skipping creation.")
        return
    elif resp_get.status_code not in (404,):
        raise Exception(f"Unexpected error checking DataScan: {resp_get.status_code} {resp_get.text}")

    # 2) Create DataScan (long-running operation)
    body = build_dq_scan_body()
    create_url = (
        f"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}"
        f"/locations/{REGION}/dataScans?dataScanId={SCAN_ID}"
    )
    resp_create = requests.post(create_url, headers=headers, data=json.dumps(body))
    print(f"Create DataScan response: {resp_create.status_code} {resp_create.text}")

    if resp_create.status_code not in (200, 201):
        raise Exception(f"Failed to create DataScan: {resp_create.status_code} {resp_create.text}")

    op_name = resp_create.json().get("name")
    if not op_name:
        # Some versions may create synchronously; if so, weâ€™re done
        print("No operation name returned, assuming DataScan is ready.")
        return

    # 3) Poll the operation until done
    op_url = f"https://dataplex.googleapis.com/v1/{op_name}"
    timeout_sec = 60 * 5
    interval_sec = 10
    deadline = time.time() + timeout_sec

    while True:
        if time.time() > deadline:
            raise TimeoutError("Timed out waiting for DataScan create operation")

        op_resp = requests.get(op_url, headers=headers)
        print(f"Operation status: {op_resp.status_code} {op_resp.text}")

        if op_resp.status_code != 200:
            raise Exception(f"Error checking operation: {op_resp.status_code} {op_resp.text}")

        op_json = op_resp.json()
        if op_json.get("done"):
            if "error" in op_json:
                raise Exception(f"Create DataScan operation error: {op_json['error']}")
            print("DataScan create operation completed.")
            return

        print("Create operation still running... waiting...")
        time.sleep(interval_sec)


def run_dq_scan(**context):
    """
    Trigger a DataScan run.
    Will only be called after create_or_update_scan, which waits until the
    DataScan exists, so 404 should no longer happen.
    """
    headers = get_auth_headers()
    url = (
        f"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}"
        f"/locations/{REGION}/dataScans/{SCAN_ID}:run"
    )

    response = requests.post(url, headers=headers)
    print(f"Run DataScan response: {response.status_code} {response.text}")

    if response.status_code not in (200, 201):
        raise Exception(f"Failed to trigger DQ scan: {response.status_code} {response.text}")


def check_dq_scan_status(**context):
    """
    Poll the jobs endpoint until the last job is SUCCEEDED / FAILED / CANCELLED.
    """
    headers = get_auth_headers()
    url = (
        f"https://dataplex.googleapis.com/v1/projects/{PROJECT_ID}"
        f"/locations/{REGION}/dataScans/{SCAN_ID}/jobs"
    )

    timeout_minutes = 15
    poll_interval_seconds = 30
    deadline = time.time() + timeout_minutes * 60

    while True:
        if time.time() > deadline:
            raise TimeoutError("Timed out waiting for DQ job to finish")

        resp = requests.get(url, headers=headers)
        print(f"Jobs status response: {resp.status_code} {resp.text}")

        if resp.status_code != 200:
            raise Exception(f"Failed to get jobs: {resp.status_code} {resp.text}")

        jobs = resp.json().get("jobs", [])
        if not jobs:
            print("No jobs yet. Sleeping...")
            time.sleep(poll_interval_seconds)
            continue

        job = jobs[0]  # latest job
        state = job.get("state")
        print(f"Latest job state: {state}")

        if state == "SUCCEEDED":
            print("DQ scan SUCCEEDED.")
            return
        elif state in ("FAILED", "CANCELLED", "ABORTED"):
            raise Exception(f"DQ scan {state}: {job}")
        else:
            print("DQ scan still running... waiting...")
            time.sleep(poll_interval_seconds)

# -------------------------------------------------------------------
# DAG
# -------------------------------------------------------------------
default_args = {
    "owner": "dataplex",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=5),
}

with DAG(
    dag_id="odin_refdata_dataplex_bq_data_quality_dag",
    start_date=datetime(2025, 11, 1),
    schedule_interval=None,
    catchup=False,
    default_args=default_args,
    tags=["dataplex", "data_quality"],
) as dag:

    create_scan_task = PythonOperator(
        task_id="create_or_update_scan",
        python_callable=create_or_update_scan,
        provide_context=True,
    )

    run_scan_task = PythonOperator(
        task_id="run_dq_scan",
        python_callable=run_dq_scan,
        provide_context=True,
    )

    check_status_task = PythonOperator(
        task_id="check_dq_scan_status",
        python_callable=check_dq_scan_status,
        provide_context=True,
    )

    create_scan_task >> run_scan_task >> check_status_task
