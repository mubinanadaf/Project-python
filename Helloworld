from datetime import datetime
import sys
from app.audits.audits import create_audits
from app.extractors.data_extractor import get_sql_connection, fetch_large_data
from app.convertors.schema_generator import generate_avro_schema
from app.convertors.convert_to_avro import convert_to_avro

def process_data_pipeline(config, query_name):
    gcs_token = get_gcs_token()
    output_path = config["paths"]["output_path"]

    # Stage 1️⃣ SQL Connection & Data Extraction
    unix_start_time = int(datetime.now().timestamp())
    extracted_file = None
    try:
        conn = get_sql_connection(config)
        Logger.info("SQL connection established successfully")

        extracted_file, columns = fetch_large_data(conn, config, query_name)
        if extracted_file == "error" or not columns:
            raise Exception("Data extraction failed")

        unix_end_time = int(datetime.now().timestamp())
        create_audits(
            output_path, gcs_token,
            [extracted_file],
            unix_start_time, unix_end_time
        )
        Logger.info("SQL extraction audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(
            output_path, gcs_token,
            [extracted_file if extracted_file else "sql_extraction"],
            unix_start_time, unix_end_time
        )
        Logger.error(f"SQL extraction failed: {e}")
        sys.exit(1)

    # Stage 2️⃣ AVRO Schema Generation
    unix_start_time = int(datetime.now().timestamp())
    try:
        generate_avro_schema(config, query_name, columns)
        unix_end_time = int(datetime.now().timestamp())

        create_audits(
            output_path, gcs_token,
            ["schema.avsc"],
            unix_start_time, unix_end_time
        )
        Logger.info("Schema generation audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(
            output_path, gcs_token,
            ["schema.avsc"],
            unix_start_time, unix_end_time
        )
        Logger.error(f"Schema generation failed: {e}")
        sys.exit(1)

    # Stage 3️⃣ Avro Conversion (avro, ctl, toc)
    unix_start_time = int(datetime.now().timestamp())
    try:
        generated_files, unix_end_time = convert_to_avro(config)
        create_audits(output_path, gcs_token, generated_files, unix_start_time, unix_end_time)
        Logger.info("Avro conversion audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, [], unix_start_time, unix_end_time)
        Logger.error(f"Avro conversion failed: {e}")
        sys.exit(1)

    Logger.info("All stages completed successfully.")





valid_extensions = [".avro", ".ctl", ".toc"]
valid_stage_names = ["SQL_CONNECTION", "DATA_EXTRACTION", "SCHEMA_GENERATION", "AVRO_CONVERSION"]

for file_name in generated_files:
    file_path = os.path.join(directory, file_name)
    file_extension = os.path.splitext(file_name)[1].lower()

    if file_extension not in valid_extensions and file_name not in valid_stage_names:
        Logger.info(f"Skipping unsupported file or stage: {file_name}")
        continue

    # safely handle missing files for stage names
    try:
        file_size = os.path.getsize(file_path)
    except FileNotFoundError:
        file_size = 0

    # …build and publish payload


.......
if item not in valid_stage_names:
    file_path = os.path.join(directory, item)
    file_extension = os.path.splitext(item)[1].lower()

    if file_extension not in valid_extensions:
        logger.info(f"Raise exception with file: {item}")
        continue

    base_name = os.path.basename(item)
    file_size = os.path.getsize(file_path)
    file_type = "file"

    action = (
        "4/4 PROCESS AVRO" if file_extension == ".avro" else
        "4/4 PROCESS CTL" if file_extension == ".ctl" else
        "4/4 PROCESS TOC" if file_extension == ".toc" else
        "4/4 PROCESS FILE"
    )
,......mm
from unittest.mock import patch, ANY
from google.api_core.exceptions import AlreadyExists

@patch("src.odin.odin_corpsec_catalog_creation_composer.assign_pii_tags")
@patch("src.odin.odin_corpsec_catalog_creation_composer.get_pii_config")
@patch("src.odin.odin_corpsec_catalog_creation_composer.get_schema_files_list")
def test_assign_pii_tags(self, mock_get_schema_files_list, mock_get_pii_config, mock_assign_pii_tags):
    # Arrange: ensure one schema file + a matching pii key
    mock_get_schema_files_list.return_value = (["period.json"], ["corpsec_customer_schema_bq.json"])
    mock_get_pii_config.return_value = {"CUSTOMER": {"email": True}}

    mock_assign_pii_tags.return_value = True   # don’t raise AlreadyExists here

    import src.odin.odin_corpsec_catalog_creation_composer as odin_corpsec_catalog_creation_composer

    # Act
    odin_corpsec_catalog_creation_composer.create_fileset()

    # Assert
    mock_assign_pii_tags.assert_called_once_with(
        "CUSTOMER",  # computed pii_key
        ANY,         # col_tag_template_name
        ANY,         # entry_name
        {"CUSTOMER": {"email": True}}
    )
