def wait_for_dq_completion(**kwargs):
    """
    1) Poll the Dataplex DataScanJob until it reaches a terminal state.
    2) If job SUCCEEDED but any rule failed, raise AirflowException so the task fails.
    """
    from airflow.exceptions import AirflowException

    ti = kwargs["ti"]

    # If run_dq_scan returns the job name, this pulls the return_value
    job_name = ti.xcom_pull(task_ids="run_dq_scan")   # or key="return_value"
    if not job_name:
        raise AirflowException("No job_name found in XCom from run_dq_scan")

    headers = get_auth_headers()

    # view=FULL is important so that dataQualityResult is included in the job JSON
    job_url = f"{DATAPLEX_BASE_URL}/{job_name}?view=FULL"
    print(f"Polling DataScanJob at: {job_url}")

    terminal_states = ["SUCCEEDED", "FAILED", "CANCELLED", "ABORTED"]
    poll_interval_sec = 30          # sleep time between checks
    max_wait_sec = 1800             # 30 minutes max
    waited = 0
    last_state = None

    while waited < max_wait_sec:
        resp = requests.get(job_url, headers=headers)
        print(f"Job GET response: {resp.status_code} {resp.text}")
        resp.raise_for_status()

        job = resp.json()
        state = job.get("state")
        last_state = state
        print(f"Current DataScanJob state = {state}")

        # ---------- TERMINAL JOB STATES ----------
        if state in terminal_states:
            # If the Dataplex job itself failed/cancelled, fail the task
            if state != "SUCCEEDED":
                raise AirflowException(
                    f"Data quality scan job finished in state {state}. Job: {job}"
                )

            # ---------- JOB SUCCEEDED: NOW CHECK RULES ----------
            dq_result = job.get("dataQualityResult", {})  # DataQualityResult
            overall_passed = dq_result.get("passed")
            rules_results = dq_result.get("rules", [])

            print(f"Overall dataQualityResult.passed = {overall_passed}")
            print(f"Total rule results = {len(rules_results)}")

            failing_rules = []
            for r in rules_results:
                if r.get("passed") is False:
                    rule_def = r.get("rule", {})
                    col = rule_def.get("column", "<table-level>")
                    desc = rule_def.get("description", "")
                    failing_rules.append(f"{col}: {desc}")

            # If overall_passed is False OR any individual rule has passed=False,
            # we mark the Airflow task as FAILED.
            if overall_passed is False or failing_rules:
                msg = (
                    "Data quality scan job SUCCEEDED but some rules FAILED. "
                    f"overall_passed={overall_passed}, "
                    f"failing_rules={failing_rules}"
                )
                raise AirflowException(msg)

            # All rules passed ✅
            print("Data quality scan SUCCEEDED and all rules PASSED.")
            ti.xcom_push(key="dq_job_result", value=job)
            return

        # ---------- STILL RUNNING ----------
        print(f"Still running… waiting {poll_interval_sec} seconds…")
        time.sleep(poll_interval_sec)
        waited += poll_interval_sec

    # ---------- TIMEOUT ----------
    raise AirflowException(
        f"Timed out after {max_wait_sec} seconds waiting for job. Last state={last_state}"
    )
