import os
import json
import logging
import uuid
from app.auth.auth_utils import pub_sub_url, publish_audit_logs

Logger = logging.getLogger(__name__)

def create_audit_payload(
    file_name, base_name, action, status, unix_start_time,
    unix_end_time, file_size, file_type, exception=None
):
    trx_id = str(uuid.uuid4())
    return {
        "uuid": str(uuid.uuid4()),
        "app_id": "odin_dev",
        "trx_id": trx_id,
        "component": "Data Processing",
        "object_name": base_name,
        "action": action,
        "pipeline_type": "data ingestion",
        "status": status,
        "message": f"{file_name} processed successfully" if status.lower() == "success" else f"{file_name} failed",
        "exception": exception if exception else "",
        "start_time": unix_start_time,
        "end_time": unix_end_time,
        "time_taken_in_sec": round(unix_end_time - unix_start_time, 2),
        "object_size_kb": {"double": round(int(file_size) / 1024, 2)},
        "object_type": file_type
    }

def create_audits(directory, gcs_token, generated_files, unix_start_time, unix_end_time):
    try:
        valid_extensions = [".avro", ".ctl", ".toc"]
        valid_stage_names = [
            "SQL_CONNECTION", "DATA_EXTRACTION",
            "SCHEMA_GENERATION", "AVRO_CONVERSION"
        ]

        for file_name in generated_files:
            file_path = os.path.join(directory, file_name)
            file_extension = os.path.splitext(file_name)[1].lower()

            if file_extension not in valid_extensions and file_name not in valid_stage_names:
                Logger.info(f"Skipping unsupported file or stage: {file_name}")
                continue

            action = "PROCESS FILE"
            exception_message = ""
            status = "SUCCESS"

            try:
                if file_name in valid_stage_names:
                    base_name = file_name
                    file_size = 0
                    file_type = "stage"
                else:
                    base_name = os.path.basename(file_path)
                    file_size = os.path.getsize(file_path)
                    file_type = file_extension.replace(".", "")

                audit_payload_dict = create_audit_payload(
                    file_name, base_name, action, status,
                    unix_start_time, unix_end_time,
                    file_size, file_type, exception_message
                )

                Logger.info(f"Publishing payload to Pub/Sub: {json.dumps(audit_payload_dict, indent=2)}")

                audit_payload = json.dumps(audit_payload_dict)
                publish_audit_logs(pub_sub_url, audit_payload, gcs_token)

                Logger.info(f"Successfully processed audit for: {file_name}")

            except Exception as e:
                status = "FAILURE"
                exception_message = str(e)
                audit_payload_dict = create_audit_payload(
                    file_name, base_name, action, status,
                    unix_start_time, unix_end_time,
                    file_size, file_type, exception_message
                )
                audit_payload = json.dumps(audit_payload_dict)
                publish_audit_logs(pub_sub_url, audit_payload, gcs_token)
                Logger.error(f"Failed to process audit for {file_name}: {e}")

    except Exception as e:
        Logger.error(f"An error occurred while creating audits: {e}")
        raise





.....

from datetime import datetime
import sys
import logging

from app.logging.logging_config import setup_logging
from app.extractors.data_extractor import get_sql_connection, fetch_large_data
from app.convertors.schema_generator import generate_avro_schema
from app.convertors.convert_to_avro import convert_to_avro
from app.audits.audits import create_audits, get_gcs_token

Logger = logging.getLogger(__name__)

def process_data_pipeline(config, query_name):
    gcs_token = get_gcs_token()
    output_path = config["paths"]["output_path"]

    # 1️⃣ SQL Connection
    unix_start_time = int(datetime.now().timestamp())
    try:
        conn = get_sql_connection(config)
        unix_end_time = int(datetime.now().timestamp())

        Logger.info("SQL connection established successfully")
        create_audits(output_path, gcs_token, ["SQL_CONNECTION"], unix_start_time, unix_end_time)
        Logger.info("SQL connection audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, ["SQL_CONNECTION"], unix_start_time, unix_end_time)
        Logger.error(f"SQL connection failed: {e}")
        sys.exit(1)

    # 2️⃣ Data Extraction
    unix_start_time = int(datetime.now().timestamp())
    try:
        extracted_file, columns = fetch_large_data(conn, config, query_name)
        if extracted_file == "error" or not columns:
            raise Exception("Data extraction failed")

        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, ["DATA_EXTRACTION"], unix_start_time, unix_end_time)
        Logger.info("Data extraction audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, ["DATA_EXTRACTION"], unix_start_time, unix_end_time)
        Logger.error(f"Data extraction failed: {e}")
        sys.exit(1)

    # 3️⃣ AVRO Schema Generation
    unix_start_time = int(datetime.now().timestamp())
    try:
        generate_avro_schema(config, query_name, columns)
        unix_end_time = int(datetime.now().timestamp())

        create_audits(output_path, gcs_token, ["SCHEMA_GENERATION"], unix_start_time, unix_end_time)
        Logger.info("Schema generation audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, ["SCHEMA_GENERATION"], unix_start_time, unix_end_time)
        Logger.error(f"Schema generation failed: {e}")
        sys.exit(1)

    # 4️⃣ Avro Conversion
    unix_start_time = int(datetime.now().timestamp())
    try:
        generated_files, unix_end_time = convert_to_avro(config)

        create_audits(output_path, gcs_token, generated_files, unix_start_time, unix_end_time)
        Logger.info("Avro conversion audit logged.")
    except Exception as e:
        unix_end_time = int(datetime.now().timestamp())
        create_audits(output_path, gcs_token, [], unix_start_time, unix_end_time)
        Logger.error(f"Avro conversion failed: {e}")
        sys.exit(1)

    Logger.info("All stages completed successfully.")



...


valid_stages = [
    "SQL_CONNECTION",
    "DATA_EXTRACTION",
    "SCHEMA_GENERATION",
    "AVRO_CONVERSION"
]

for item in generated_items:
    ...
    if item in valid_stages:
        base_name = item
        file_type = "stage"
        action = (
            "CONNECT TO SQL" if item == "SQL_CONNECTION"
            else "EXTRACT DATA" if item == "DATA_EXTRACTION"
            else "GENERATE SCHEMA" if item == "SCHEMA_GENERATION"
            else "CONVERT TO AVRO" if item == "AVRO_CONVERSION"
            else "PROCESS FILE"
        )
    else:
        base_name = os.path.basename(file_path)
        file_size = os.path.getsize(file_path)
        file_type = file_extension.replace(".", "")
        action = "PROCESS FILE"
