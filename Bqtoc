{
  "dataTransfered": {
    "uuid": "dummy-uuid-for-bq2bq",
    "businessProcessingDateTime": "20250715T045359Z",
    "fileNameConfiguredInCDMP": "t_badge_events",
    "fileType": "avro",
    "datafile": [
      {
        "name": "t_badge_events"
      }
    ]
  }
}

...
def list_objects_to_process(**context):
    toc_dict = {}
    try:
        config_dict = context['ti'].xcom_pull(task_ids='cdp_config_file_validation')['config']
        logging.info(config_dict)

        for key, value in config_dict.items():
            source_type = value['source_name']
            src_bkt = value['source_object'].replace("gs://", "")
            
            # ✅ Skip TOC processing for BigQuery source
            if source_type == "BigQuery":
                logging.info(f"Skipping TOC load for BigQuery source: {key}")
                toc_dict[key] = {
                    'toc_data': {},
                    'toc_files': [],
                    'toc_list': [],
                    'src_bkt': None
                }
                continue

            # ✅ Continue GCS TOC processing
            storage_client = storage.Client()
            blobs = storage_client.list_blobs(src_bkt)
            toc_ptrn = value['toc_file']

            toc_blobs = [blob for blob in blobs if toc_ptrn in blob.name and blob.name.endswith('toc')]
            toc_files = [blob.name for blob in toc_blobs]

            logging.info(toc_files)

            toc_list = [toc_ptrn]  # optionally can use list of matching files

            toc_dict[key] = process_toc_list(toc_list, src_bkt, toc_ptrn, key, value, context, toc_dict)

    except Exception as ex:
        logging.error(f"An error has occurred in the process: {ex}")
        sys.exit(1)

    return toc_dict
........
def process_toc_list(toc_list, src_bkt, toc_ptrn, key, value, context, toc_dict):
    storage_client = storage.Client()
    trx_id = context['ti'].xcom_pull(task_ids='cdp_config_file_validation')['trx_id']
    object_size = 0

    # If toc_list is empty (BQ-to-BQ scenario), populate toc_dict directly
    if not toc_list:
        logging.info("TOC list is empty — assuming BQ to BQ load.")
        uvid_value = get_uuid({})
        toc_dict[key] = {
            'uvid': uvid_value,
            'trx_id': trx_id,
            'config_key': key,
            'config_data': value,
            'object_size_kb': 0
        }
        return toc_dict, object_size

    # For GCS-to-BQ load using TOC
    for file_data in toc_list:
        if file_data["dataTransfered"]["fileNameConfiguredInCDMP"] == toc_ptrn:
            src_bucket = storage_client.get_bucket(src_bkt)
            blob = src_bucket.get_blob(file_data["dataTransfered"]["dataFile"])

            if blob:
                object_size += blob.size
                object_size_kb = round(object_size / 1024, 2)

                avro_files_dict = read_gcs_file(src_bkt, file_data["dataTransfered"]["dataFile"])
                file_type = read_gcs_file(src_bkt, file_data["dataTransfered"]["fileType"], "avro").lower()

                avro_files = [item["name"] for item in avro_files_dict]
                logging.info(f"List of data files: {avro_files}")

                ctl_files = [file.replace(file_type, 'ctl') for file in avro_files]
                logging.info(f"List of CTL files: {ctl_files}")

                uvid_value = get_uuid(file_data)

                ctl_all_files, ctl_missing_files = check_ctl_files(ctl_files, src_bkt)
                if ctl_missing_files:
                    context['ti'].xcom_push(key='ctl_missing_files', value=ctl_missing_files)
                    logging.info(f"TOC file {file_data} available but corresponding CTL files are missing: {ctl_missing_files}")

                if len(ctl_all_files) == len(ctl_files):
                    toc_dict[key] = {
                        'uvid': uvid_value,
                        'trx_id': trx_id,
                        'config_key': key,
                        'config_data': value,
                        'object_size_kb': object_size_kb
                    }

    return toc_dict, object_size
