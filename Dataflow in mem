from datetime import datetime, timedelta

from airflow import DAG
from airflow.providers.google.cloud.operators.dataflow import (
    DataflowCreatePythonJobOperator,
)

PROJECT_ID = "sandbox-corp-odin-dev1-f930"
REGION = "us-central1"

# Where you uploaded in_memory_square_pipeline.py
GCS_PYTHON_FILE = (
    "gs://us-central1-cm-v3_sandbox-corp-odin-dev1-f930/dags/odin/common_functions/"
    "in_memory_square_pipeline.py"
)

# >>> Your temp & staging locations <<<
TEMP_LOCATION = "gs://nonsrd1_sandbox-corp-odin-dev1-f930/dataflow/temp"
STAGING_LOCATION = "gs://nonsrd1_sandbox-corp-odin-dev1-f930/dataflow/staging"

default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "retries": 0,
    "retry_delay": timedelta(minutes=2),
}

with DAG(
    dag_id="in_memory_square_dataflow_dag",
    start_date=datetime(2025, 1, 1),
    schedule_interval=None,  # trigger manually
    catchup=False,
    default_args=default_args,
    tags=["dataflow", "in-memory"],
) as dag:

    in_memory_square = DataflowCreatePythonJobOperator(
        task_id="run_in_memory_square",
        py_file=GCS_PYTHON_FILE,
        py_options=[],  # e.g. ["-m"] if you use module mode
        job_name="in-memory-square-{{ ds_nodash }}",
        location=REGION,
        options={
            # Dataflow / Beam options
            "project": PROJECT_ID,
            "region": REGION,
            "runner": "DataflowRunner",
            "temp_location": TEMP_LOCATION,
            "staging_location": STAGING_LOCATION,

            # Our custom pipeline option
            "input_value": 9,  # change this number to test
        },
        py_requirements=["apache-beam[gcp]==2.59.0"],
        py_interpreter="python3",
        py_system_site_packages=False,
        wait_until_finished=True,
    )
