# tests/test_odin_bq_bq_data_load_dag.py
import os
import sys
import types
import importlib
import unittest
from unittest.mock import MagicMock


class TestBqToBqDataLoadDag(unittest.TestCase):
    def setUp(self):
        # ------------------------------------------------------------------
        # 1) Mock your utils package and its submodules BEFORE importing DAG
        # ------------------------------------------------------------------
        # Create a fake 'utils' package
        utils_pkg = types.ModuleType("utils")
        sys.modules["utils"] = utils_pkg

        # utils.cdp_audit_logs
        self.mock_log_audit = MagicMock(name="log_task_level_audit")
        cdp_audit_logs = types.ModuleType("utils.cdp_audit_logs")
        cdp_audit_logs.log_task_level_audit = self.mock_log_audit
        sys.modules["utils.cdp_audit_logs"] = cdp_audit_logs

        # utils.cdp_common_functions
        self.mock_run_time = MagicMock(name="run_time_variables")
        self.mock_pre      = MagicMock(name="dimc_pre_check")
        self.mock_write    = MagicMock(name="data_load_write_disposition")
        self.mock_load     = MagicMock(name="data_load_task")
        self.mock_post     = MagicMock(name="dimc_post_check")

        cdp_common_functions = types.ModuleType("utils.cdp_common_functions")
        cdp_common_functions.run_time_variables          = self.mock_run_time
        cdp_common_functions.dimc_pre_check              = self.mock_pre
        cdp_common_functions.data_load_write_disposition = self.mock_write
        cdp_common_functions.data_load_task              = self.mock_load
        cdp_common_functions.dimc_post_check             = self.mock_post
        sys.modules["utils.cdp_common_functions"] = cdp_common_functions

        # ---------------------------------------------------------------
        # 2) Mock airflow modules (no real Airflow needed in the tests)
        # ---------------------------------------------------------------
        class FakeDAG:
            def __init__(self, **kwargs):
                self.dag_id = kwargs.get("dag_id")
                self.schedule_interval = kwargs.get("schedule_interval")
                self.catchup = kwargs.get("catchup", True)
                self.max_active_runs = kwargs.get("max_active_runs", 16)
                self.default_args = kwargs.get("default_args", {})
                self.tasks = []

            def add_task(self, task):
                self.tasks.append(task)

            def get_task(self, task_id):
                for t in self.tasks:
                    if t.task_id == task_id:
                        return t
                raise KeyError(task_id)

        class FakePythonOperator:
            def __init__(
                self,
                *,
                task_id,
                python_callable=None,
                on_success_callback=None,
                on_failure_callback=None,
                dag=None,
                **_
            ):
                self.task_id = task_id
                self.python_callable = python_callable
                self.on_success_callback = on_success_callback
                self.on_failure_callback = on_failure_callback
                self.upstream_task_ids = set()
                self.downstream_task_ids = set()
                if dag is not None and hasattr(dag, "add_task"):
                    dag.add_task(self)

            # a >> b
            def __rshift__(self, other):
                self.set_downstream(other)
                return other

            # b << a
            def __lshift__(self, other):
                self.set_upstream(other)
                return other

            def set_upstream(self, task):
                self.upstream_task_ids.add(task.task_id)
                task.downstream_task_ids.add(self.task_id)

            def set_downstream(self, task):
                self.downstream_task_ids.add(task.task_id)
                task.upstream_task_ids.add(self.task_id)

        fake_airflow                 = types.ModuleType("airflow")
        fake_airflow_models          = types.ModuleType("airflow.models")
        fake_airflow_utils           = types.ModuleType("airflow.utils")
        fake_airflow_utils_dates     = types.ModuleType("airflow.utils.dates")
        fake_airflow_ops             = types.ModuleType("airflow.operators")
        fake_airflow_ops_python      = types.ModuleType("airflow.operators.python")

        # Build a FakeDAG instance via a constructor so we can capture it
        self._dag_instance = None
        def _dag_ctor(**kwargs):
            self._dag_instance = FakeDAG(**kwargs)
            return self._dag_instance

        fake_airflow.DAG = MagicMock(side_effect=_dag_ctor)
        fake_airflow_utils_dates.days_ago = MagicMock(return_value=0)
        fake_airflow_ops_python.PythonOperator = FakePythonOperator

        sys.modules["airflow"] = fake_airflow
        sys.modules["airflow.models"] = fake_airflow_models
        sys.modules["airflow.utils"] = fake_airflow_utils
        sys.modules["airflow.utils.dates"] = fake_airflow_utils_dates
        sys.modules["airflow.operators"] = fake_airflow_ops
        sys.modules["airflow.operators.python"] = fake_airflow_ops_python

        self._fake_airflow = fake_airflow  # keep for asserting DAG ctor args later

        # ---------------------------------------------------------------
        # 3) Ensure 'src' on path and import the DAG module under test
        # ---------------------------------------------------------------
        if "PYTHONPATH" not in os.environ:
            sys.path.insert(0, os.path.abspath("src"))

        # Adjust module path if your file name differs
        self.mod_path = "odin.dag.odin_bq_bq_data_load"
        self.mod = importlib.import_module(self.mod_path)
        self.dag = self._dag_instance

    def tearDown(self):
        for k in [
            "utils",
            "utils.cdp_audit_logs",
            "utils.cdp_common_functions",
            "airflow",
            "airflow.models",
            "airflow.utils",
            "airflow.utils.dates",
            "airflow.operators",
            "airflow.operators.python",
            self.mod_path,
        ]:
            sys.modules.pop(k, None)

    # ------------------------------ Tests ------------------------------

    def test_import_ok(self):
        try:
            importlib.reload(self.mod)
        except Exception as e:
            self.fail(f"DAG import raised exception: {e!r}")

    def test_dag_constructor_args(self):
        kwargs = self._fake_airflow.DAG.call_args.kwargs
        self.assertEqual(kwargs.get("dag_id"), "odin_bq_bq_data_load")
        self.assertIsNone(kwargs.get("schedule_interval"))
        self.assertFalse(kwargs.get("catchup", True))
        self.assertEqual(kwargs.get("max_active_runs"), 25)
        defaults = kwargs.get("default_args", {})
        self.assertIn("start_date", defaults)
        self.assertEqual(defaults.get("retries"), 3)

    def test_tasks_exist_and_callables(self):
        expected = {
            "run_time_variables": self.mock_run_time,
            "dimc_pre_check": self.mock_pre,
            "data_load_write_disposition": self.mock_write,
            "data_load_task": self.mock_load,
            "dimc_post_check": self.mock_post,
        }
        got_ids = {t.task_id for t in self.dag.tasks}
        self.assertEqual(set(expected.keys()), got_ids)

        for tid, fn in expected.items():
            op = self.dag.get_task(tid)
            self.assertIs(op.python_callable, fn, f"{tid} wired to wrong callable")

    def test_dependency_chain(self):
        rt   = self.dag.get_task("run_time_variables")
        pre  = self.dag.get_task("dimc_pre_check")
        wd   = self.dag.get_task("data_load_write_disposition")
        dl   = self.dag.get_task("data_load_task")
        post = self.dag.get_task("dimc_post_check")

        self.assertEqual(rt.downstream_task_ids, {"dimc_pre_check"})
        self.assertEqual(pre.upstream_task_ids, {"run_time_variables"})
        self.assertEqual(pre.downstream_task_ids, {"data_load_write_disposition"})
        self.assertEqual(wd.upstream_task_ids, {"dimc_pre_check"})
        self.assertEqual(wd.downstream_task_ids, {"data_load_task"})
        self.assertEqual(dl.upstream_task_ids, {"data_load_write_disposition"})
        self.assertEqual(dl.downstream_task_ids, {"dimc_post_check"})
        self.assertEqual(post.upstream_task_ids, {"data_load_task"})
        self.assertEqual(post.downstream_task_ids, set())

    def test_callbacks(self):
        # run_time_variables typically has no callbacks; others should use audit callback
        rt = self.dag.get_task("run_time_variables")
        self.assertIsNone(rt.on_success_callback)
        self.assertIsNone(rt.on_failure_callback)

        for tid in ["dimc_pre_check", "data_load_write_disposition", "data_load_task", "dimc_post_check"]:
            t = self.dag.get_task(tid)
            self.assertIs(t.on_success_callback, self.mock_log_audit)
            self.assertIs(t.on_failure_callback, self.mock_log_audit)


if __name__ == "__main__":
    unittest.main()
# BEFORE you do: fake_airflow_ops_python.PythonOperator = FakePythonOperator
# make sure the class is already defined:

class FakePythonOperator:
    def __init__(self, *, task_id, python_callable=None,
                 on_success_callback=None, on_failure_callback=None,
                 dag=None, **_):
        self.task_id = task_id
        self.python_callable = python_callable
        self.on_success_callback = on_success_callback
        self.on_failure_callback = on_failure_callback
        self.upstream_task_ids, self.downstream_task_ids = set(), set()
        if dag is not None and hasattr(dag, "add_task"):
            dag.add_task(self)

    # ✅ support a >> [b, c] and [b, c] << a (fixes dependency test failures)
    def __rshift__(self, other):
        items = other if isinstance(other, (list, tuple, set)) else [other]
        for t in items:
            self.set_downstream(t)
        return other

    def __lshift__(self, other):
        items = other if isinstance(other, (list, tuple, set)) else [other]
        for t in items:
            self.set_upstream(t)
        return other

    def set_upstream(self, task):
        self.upstream_task_ids.add(task.task_id)
        task.downstream_task_ids.add(self.task_id)

    def set_downstream(self, task):
        self.downstream_task_ids.add(task.task_id)
        task.upstream_task_ids.add(self.task_id)

# …then:
fake_airflow_ops_python.PythonOperator = FakePythonOperator
