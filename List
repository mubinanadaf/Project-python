toc_dict = {}

try:
    config_dict = context['ti'].xcom_pull(task_ids='cdp_config_file_validation')['config']
    Logging.info(config_dict)

    for key, value in config_dict.items():
        source_type = value.get("source_type", "").lower()
        if source_type == "bigquery":
            Logging.info(f"Skipping .toc processing for BigQuery source: {key}")
            toc_dict[key] = {"bq_source": True}
            continue  # Skip the GCS toc block

        src_bkt = value['source_object'].replace("gs://", "")
        toc_ptrn = value['source_name']
        storage_client = storage.Client()
        gcs_blobs = storage_client.list_blobs(src_bkt)
        toc_blobs = [blob for blob in gcs_blobs if toc_ptrn in blob.name and blob.name.endswith('toc')]
        toc_files = [blob.name for blob in toc_blobs]

        Logging.info(f"Found toc files: {toc_files}")
        toc_list = [(toc_file, read_gcs_file(src_bkt, toc_file)) for toc_file in toc_files]
        toc_dict = process_toc_list(toc_list, src_bkt, toc_ptrn, key, True, context, toc_dict)

except Exception as e:
    logging.error(f"An error has occurred in the process ::: {e}")
    sys.exit(1)

return toc_dict

..
...
.

def sf_validate_pre_checks(**kwargs):
    from datetime import datetime as dt2, timezone
    import sys
    import logging

    ti = kwargs['ti']
    pre_check_response = {}

    try:
        sfr_project_id = ti.xcom_pull(task_ids='run_time_variables', key='sfr_project_id')
        config_data = ti.xcom_pull(task_ids='run_time_variables', key='config_data')
        sfr_outbound_dataset = ti.xcom_pull(task_ids='run_time_variables', key='sfr_outbound_dataset')
        part_tables = sfr_outbound_dataset.split(',') if sfr_outbound_dataset else []

        # Get toc_file - this is used only for GCS
        toc_file = ti.xcom_pull(task_ids='run_time_variables', key='toc_file')

        # Determine source type
        source_type = config_data.get('source_type', '').lower()

        # GCS Source: Read avro_files_dict using GCS logic
        if source_type == 'gcs':
            bucket = config_data['source_object'].replace('gs://', '')
            avro_files_dict = list_gcs_file(bucket, toc_file)["dataTransferred"]["dataFile"]
        else:
            logging.info("BigQuery source detected. Skipping GCS file reading.")
            avro_files_dict = []  # Dummy fallback

        avro_files = [item["name"] for item in avro_files_dict] if avro_files_dict else []

        # Prepare response metadata
        pre_check_response["trx_id"] = ti.xcom_pull(task_ids='run_time_variables', key='trx_id')
        pre_check_response["uuid"] = ti.xcom_pull(task_ids='run_time_variables', key='uuid')
        pre_check_response["project_id"] = config_data["project_id"]
        pre_check_response["app_id"] = config_data["app_id"] + "_composer_job"
        pre_check_response["created_by"] = "control_type: pre-controls"
        pre_check_response["control_type"] = "bigquery"
        pre_check_response["source_name"] = config_data["source_name"]
        pre_check_response["source_object"] = config_data["sfr_outbound_dataset"]
        pre_check_response["target_name"] = config_data["target_name"]
        pre_check_response["target_object"] = config_data["target_object"]

        logging.info(f"List of data files: {avro_files}")

        # Start pre-control checks
        missing_staging_tables = []
        empty_tables = []
        staging_tables = []

        for part_table in part_tables:
            is_table_available = sf_table_availability_check(staging_dataset, part_table)
            if is_table_available:
                is_table_not_empty = sf_table_emptiness_check(staging_dataset, part_table)
                if is_table_not_empty:
                    staging_tables.append(part_table)
                else:
                    empty_tables.append(part_table)
            else:
                missing_staging_tables.append(part_table)

        # Failure: Missing tables
        if missing_staging_tables:
            pre_check_response["control_id"] = "DQ_TAVC"
            pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
            pre_check_response["control_status"] = "failed"
            pre_check_response["log_info"] = f"tables not available - {missing_staging_tables}"
            pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
            kwargs['ti'].xcom_push(key='missing_staging_tables', value=missing_staging_tables)
            sf_publish_logs(sfr_project_id, lane, pre_check_response)
            kwargs['ti'].xcom_push(key="dimc_pre_check_response", value={
                "DQ_TABLE_AVAILABILITY_CHECK": {
                    "Execution": "performed",
                    "Result": pre_check_response["log_info"]
                }
            })
            sys.exit(1)

        # Failure: Empty tables
        if empty_tables:
            pre_check_response["control_id"] = "DQ_TEMC"
            pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
            pre_check_response["control_status"] = "failed"
            pre_check_response["log_info"] = f"tables empty - {empty_tables}"
            pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
            kwargs['ti'].xcom_push(key='empty_tables', value=empty_tables)
            sf_publish_logs(sfr_project_id, lane, pre_check_response)
            kwargs['ti'].xcom_push(key="dimc_pre_check_response", value={
                "DQ_TABLE_EMPTY_CHECK": {
                    "Execution": "performed",
                    "Result": pre_check_response["log_info"]
                }
            })
            sys.exit(1)

        # Success
        pre_check_response["control_id"] = "DQ_TEVC"
        pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
        pre_check_response["control_status"] = "success"
        pre_check_response["log_info"] = f"all the staging tables having data in sf outbound dataset"
        pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
        sf_publish_logs(sfr_project_id, lane, pre_check_response)
        kwargs['ti'].xcom_push(key="dimc_pre_check_response", value={
            "DQ_TABLE_EMPTY_CHECK": {
                "Execution": "performed",
                "Result": "success"
            }
        })
        kwargs['ti'].xcom_push(key="staging_tables", value=staging_tables)

    except Exception as e:
        logging.info(f"Exception occurred while trying to execute sf_validate_pre_checks with exception: {str(e)}")
        kwargs['ti'].xcom_push(key="dimc_pre_check_response", value={"DQ_TABLE_AVAILABILITY_CHECK": {
            "Execution": "failed", "Result": str(e)}})

.......
def sf_validate_pre_checks(**kwargs): from datetime import datetime as dt2, timezone

ti = kwargs['ti']
pre_check_response = {}
pre_check_response["start_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)

sfr_project_id = ti.xcom_pull(task_ids='run_time_variables', key='sfr_project_id')
config_data = ti.xcom_pull(task_ids='run_time_variables', key='config_data')
staging_dataset = ti.xcom_pull(task_ids='run_time_variables', key='sfr_outbound_dataset').split('.')[-1]
avro_files = ti.xcom_pull(task_ids='run_time_variables', key='toc_file').get("bucket_toc", {}).get("dataTransferred", {}).get("dataFile", [])

source_type = config_data.get("source_type", "gcs").lower()
part_tables = []

if source_type == "bigquery":
    Logging.info("BigQuery Source detected. Extracting part_tables from sfr_outbound_dataset")
    part_tables = config_data.get("sfr_outbound_dataset", "").split('.')[-1].split(',')
else:
    part_tables = [file.replace('.avro', '') for file in avro_files]

pre_check_response["trx_id"] = ti.xcom_pull(task_ids='run_time_variables', key='uuid')
pre_check_response["project_id"] = config_data['project_id']
pre_check_response["app_id"] = config_data['app_id'] + "_composer_job"
pre_check_response["created_by"] = config_data['app_id'] + "_composer_job"
pre_check_response["source_name"] = config_data['source_name']
pre_check_response["source_object"] = config_data['sfr_outbound_dataset']
pre_check_response["target_name"] = config_data['target_name']
pre_check_response["target_object"] = config_data['target_object']

results = []
staging_tables = []
empty_tables = []
missing_staging_tables = []

for part_table in part_tables:
    table_id = f"{staging_dataset}.{part_table}"
    is_table_available = sf_table_availability_check(staging_dataset, part_table)

    if is_table_available:
        is_table_not_empty = sf_table_emptiness_check(staging_dataset, part_table)
        if is_table_not_empty:
            staging_tables.append(part_table)
        else:
            empty_tables.append(part_table)
    else:
        missing_staging_tables.append(part_table)

if missing_staging_tables:
    pre_check_response["control_id"] = "DQ_IAVC"
    pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
    pre_check_response["control_status"] = "failed"
    pre_check_response["log_info"] = f"tables not available - {missing_staging_tables}"
    pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
    kwargs['ti'].xcom_push(key="missing_staging_tables", value=missing_staging_tables)
    Logging.info(f"tables not available - {missing_staging_tables}")
    sf_publish_logs(sfr_project_id, "lane", pre_check_response)
    kwargs['ti'].xcom_push(key="dimc_pre_check_response", value=pre_check_response)
    sys.exit(1)
else:
    pre_check_response["control_id"] = "DQ_IAVC"
    pre_check_response["control_name"] = "DQ_TABLE_AVAILABILITY_CHECK"
    pre_check_response["control_status"] = "success"
    pre_check_response["log_info"] = "all the staging tables available in the sf outbound dataset"
    pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
    sf_publish_logs(sfr_project_id, "lane", pre_check_response)
    kwargs['ti'].xcom_push(key="dimc_pre_check_response", value=pre_check_response)

if empty_tables:
    pre_check_response["control_id"] = "DQ_TEMC"
    pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
    pre_check_response["control_status"] = "failed"
    pre_check_response["log_info"] = f"tables empty - {empty_tables}"
    pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
    kwargs['ti'].xcom_push(key="empty_tables", value=empty_tables)
    Logging.info(f"tables empty - {empty_tables}")
    sf_publish_logs(sfr_project_id, "lane", pre_check_response)
    kwargs['ti'].xcom_push(key="dimc_pre_check_response", value=pre_check_response)
    sys.exit(1)
else:
    pre_check_response["control_id"] = "DQ_TEVC"
    pre_check_response["control_name"] = "DQ_TABLE_EMPTY_CHECK"
    pre_check_response["control_status"] = "success"
    pre_check_response["log_info"] = "all the staging tables having data in sf outbound dataset"
    pre_check_response["end_time"] = int(dt2.now(timezone.utc).timestamp() * 1000)
    sf_publish_logs(sfr_project_id, "lane", pre_check_response)
    kwargs['ti'].xcom_push(key="dimc_pre_check_response", value=pre_check_response)
    kwargs['ti'].xcom_push(key="staging_tables", value=staging_tables)


        sys.exit(1)
